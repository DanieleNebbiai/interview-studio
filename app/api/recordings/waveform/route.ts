import { NextRequest, NextResponse } from "next/server";
import { join } from "path";
import { tmpdir } from "os";
import { writeFileSync, unlinkSync, existsSync, readFileSync } from "fs";
import ffmpeg from "fluent-ffmpeg";
import ffmpegStatic from "ffmpeg-static";

// Get FFmpeg path dynamically
const getFfmpegPath = () => {
  console.log('DEBUG: Getting FFmpeg path...')
  console.log('DEBUG: process.cwd():', process.cwd())

  // Skip the import completely and build path dynamically
  const path = require('path');
  const possiblePaths = [
    path.join(process.cwd(), 'node_modules', 'ffmpeg-static', 'ffmpeg'),
    path.join(__dirname, '..', '..', '..', 'node_modules', 'ffmpeg-static', 'ffmpeg'),
    path.join(__dirname, '..', '..', '..', '..', 'node_modules', 'ffmpeg-static', 'ffmpeg'),
    path.join(__dirname, '..', '..', '..', '..', '..', 'node_modules', 'ffmpeg-static', 'ffmpeg'),
  ];

  console.log('DEBUG: Trying paths:', possiblePaths)

  for (const possiblePath of possiblePaths) {
    console.log(`DEBUG: Checking path: ${possiblePath}, exists: ${existsSync(possiblePath)}`)
    if (existsSync(possiblePath)) {
      console.log('DEBUG: Found FFmpeg at:', possiblePath)
      return possiblePath;
    }
  }

  console.log('DEBUG: No FFmpeg found in any path')
  return null;
};

const ffmpegPath = getFfmpegPath();

// Configure ffmpeg to use the static binary
if (ffmpegPath) {
  console.log('Setting FFmpeg path:', ffmpegPath)
  ffmpeg.setFfmpegPath(ffmpegPath)
} else {
  console.error('FFmpeg path not found!')
}

interface Recording {
  id: string;
  recording_url?: string;
  downloadUrl?: string;
  duration: number;
}

interface WaveformPoint {
  time: number;
  amplitude: number;
}

interface WaveformData {
  recordingId: string;
  points: WaveformPoint[];
  sampleRate: number;
  duration: number;
}

export async function POST(req: NextRequest) {
  try {
    const { roomId, recordings, convertedAudioData } = await req.json();

    if (!roomId || !recordings?.length) {
      return NextResponse.json(
        { error: "roomId and recordings are required" },
        { status: 400 }
      );
    }

    console.log(`Processing waveform for ${recordings.length} recordings in room ${roomId}`);

    const waveformResults: WaveformData[] = [];
    let processedCount = 0;

    for (const recording of recordings) {
      try {
        console.log(`Processing waveform for recording ${recording.id}...`);

        // Check if we have pre-converted audio data
        const convertedAudio = convertedAudioData?.find((audio: any) => audio.recordingId === recording.id)?.audioBuffer;
        let audioBuffer: Buffer | undefined;

        if (convertedAudio) {
          // Convert base64 string back to Buffer if needed
          audioBuffer = Buffer.isBuffer(convertedAudio) ? convertedAudio : Buffer.from(convertedAudio, 'base64');
          console.log(`Found pre-converted audio for ${recording.id}, size: ${audioBuffer.length} bytes`);
        }

        // Extract waveform data from audio file
        const waveformData = await extractWaveformFromAudio(recording, audioBuffer);
        waveformResults.push(waveformData);
        processedCount++;

        console.log(`Waveform extracted for recording ${recording.id}`);
      } catch (error) {
        console.error(`Error processing waveform for recording ${recording.id}:`, error);
        // Continue with other recordings even if one fails
      }
    }

    return NextResponse.json({
      success: true,
      processedCount,
      waveforms: waveformResults,
      roomId,
    });

  } catch (error) {
    console.error("Waveform processing error:", error);
    return NextResponse.json(
      {
        error: "Failed to process waveforms",
        details: error instanceof Error ? error.message : "Unknown error",
      },
      { status: 500 }
    );
  }
}

async function extractWaveformFromAudio(recording: Recording, convertedAudio?: Buffer): Promise<WaveformData> {
  try {
    let audioBuffer: Buffer;

    if (convertedAudio) {
      // Use pre-converted audio from transcription process (MP3 16kHz mono)
      console.log(`‚úÖ TRANSCRIPTION AUDIO: Using pre-converted MP3 from transcribe API: ${recording.id}`);
      console.log(`Pre-converted audio size: ${convertedAudio.length} bytes`);

      // The convertedAudio is already MP3, decode it directly
      try {
        const { default: audioDecode } = await import('audio-decode');
        const decodedBuffer = await audioDecode(convertedAudio);

        console.log(`Transcription audio decoded:`);
        console.log(`- Sample rate: ${decodedBuffer.sampleRate} Hz`);
        console.log(`- Channels: ${decodedBuffer.numberOfChannels}`);
        console.log(`- Duration: ${decodedBuffer.duration} seconds`);

        // Convert to our WAV format for analysis
        audioBuffer = await convertAudioBufferToWAV(decodedBuffer, recording.id);
        console.log(`Converted transcription audio to WAV format: ${audioBuffer.length} bytes`);

      } catch (error) {
        console.error(`Failed to decode transcription audio:`, error);
        // Fallback to statistical analysis of the MP3 data
        audioBuffer = convertedAudio;
      }
    } else {
      // Fallback: extract real audio peaks using FFmpeg to WAV
      const audioUrl = recording.downloadUrl || recording.recording_url;

      if (!audioUrl) {
        throw new Error(`No audio URL available for recording ${recording.id}`);
      }

      console.log(`üéµ ATTEMPTING REAL AUDIO DECODE from: ${audioUrl}`);
      audioBuffer = await extractRealAudioPeaksWithDecode(audioUrl, recording.id, recording.duration);
      console.log(`üéµ REAL AUDIO DECODE COMPLETED, buffer size: ${audioBuffer.length} bytes`);
    }

    // Analyze the audio buffer
    const waveformData = await analyzeAudioBuffer(audioBuffer, recording);

    return waveformData;
  } catch (error) {
    console.error(`Error analyzing audio for recording ${recording.id}:`, error);

    // Fallback to simulated data if real analysis fails
    console.log(`Falling back to simulated waveform for recording ${recording.id}`);
    return generateSimulatedWaveform(recording);
  }
}

// Convert AudioBuffer to WAV format for analysis
async function convertAudioBufferToWAV(audioBuffer: AudioBuffer, recordingId: string): Promise<Buffer> {
  // Use the same downsampling logic as extractRealAudioPeaksWithDecode
  const channelData = audioBuffer.getChannelData(0); // First channel (mono)
  const downsampleRate = 10; // Take every 10th sample for efficiency
  const downsampledLength = Math.floor(channelData.length / downsampleRate);

  // Create a buffer with our downsampled audio data
  const audioDataBuffer = Buffer.alloc(downsampledLength * 2); // 16-bit samples

  for (let i = 0; i < downsampledLength; i++) {
    const sampleIndex = i * downsampleRate;
    const sample = channelData[sampleIndex];

    // Convert float32 (-1.0 to 1.0) to 16-bit signed integer
    const intSample = Math.max(-32768, Math.min(32767, Math.floor(sample * 32767)));

    // Write as little-endian 16-bit
    audioDataBuffer.writeInt16LE(intSample, i * 2);
  }

  // Create WAV header
  const wavHeader = Buffer.alloc(44);
  const writeString = (offset: number, str: string) => {
    for (let i = 0; i < str.length; i++) {
      wavHeader[offset + i] = str.charCodeAt(i);
    }
  };

  // WAV header
  writeString(0, 'RIFF');
  wavHeader.writeUInt32LE(36 + audioDataBuffer.length, 4);
  writeString(8, 'WAVE');
  writeString(12, 'fmt ');
  wavHeader.writeUInt32LE(16, 16); // fmt chunk size
  wavHeader.writeUInt16LE(1, 20);  // PCM format
  wavHeader.writeUInt16LE(1, 22);  // mono
  wavHeader.writeUInt32LE(Math.floor(audioBuffer.sampleRate / downsampleRate), 24); // sample rate
  wavHeader.writeUInt32LE(Math.floor(audioBuffer.sampleRate / downsampleRate) * 2, 28); // byte rate
  wavHeader.writeUInt16LE(2, 32);  // block align
  wavHeader.writeUInt16LE(16, 34); // bits per sample
  writeString(36, 'data');
  wavHeader.writeUInt32LE(audioDataBuffer.length, 40);

  // Combine header + data
  return Buffer.concat([wavHeader, audioDataBuffer]);
}

// Extract real audio peaks using audio-decode library
async function extractRealAudioPeaksWithDecode(audioUrl: string, recordingId: string, duration: number): Promise<Buffer> {
  console.log(`Downloading and decoding audio file: ${audioUrl}`);

  try {
    // Download the audio file
    const response = await fetch(audioUrl);
    if (!response.ok) {
      throw new Error(`Failed to fetch audio file: ${response.status} ${response.statusText}`);
    }

    const audioArrayBuffer = await response.arrayBuffer();
    console.log(`Downloaded ${audioArrayBuffer.byteLength} bytes, decoding audio...`);

    // Decode audio using audio-decode with dynamic import
    console.log(`Attempting to import audio-decode...`);
    const { default: audioDecode } = await import('audio-decode');
    console.log(`audio-decode imported successfully, typeof:`, typeof audioDecode);

    console.log(`Calling audioDecode with ${audioArrayBuffer.byteLength} bytes...`);
    const audioBuffer = await audioDecode(audioArrayBuffer);
    console.log(`audioDecode completed successfully`);

    console.log(`Audio decoded successfully:`);
    console.log(`- Sample rate: ${audioBuffer.sampleRate} Hz`);
    console.log(`- Channels: ${audioBuffer.numberOfChannels}`);
    console.log(`- Duration: ${audioBuffer.duration} seconds`);
    console.log(`- Length: ${audioBuffer.length} samples`);

    // Convert AudioBuffer to our format for analysis
    // We'll work with the first channel (mono) and downsample for efficiency
    const channelData = audioBuffer.getChannelData(0); // First channel
    const downsampleRate = 10; // Take every 10th sample for efficiency
    const downsampledLength = Math.floor(channelData.length / downsampleRate);

    // Create a buffer with our downsampled audio data
    const audioDataBuffer = Buffer.alloc(downsampledLength * 2); // 16-bit samples

    for (let i = 0; i < downsampledLength; i++) {
      const sampleIndex = i * downsampleRate;
      const sample = channelData[sampleIndex];

      // Convert float32 (-1.0 to 1.0) to 16-bit signed integer
      const intSample = Math.max(-32768, Math.min(32767, Math.floor(sample * 32767)));

      // Write as little-endian 16-bit
      audioDataBuffer.writeInt16LE(intSample, i * 2);
    }

    console.log(`Downsampled to ${downsampledLength} samples (${audioDataBuffer.length} bytes)`);

    // Create a fake WAV header for compatibility with our existing analysis code
    const wavHeader = Buffer.alloc(44);
    const writeString = (offset: number, str: string) => {
      for (let i = 0; i < str.length; i++) {
        wavHeader[offset + i] = str.charCodeAt(i);
      }
    };

    // WAV header
    writeString(0, 'RIFF');
    wavHeader.writeUInt32LE(36 + audioDataBuffer.length, 4);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    wavHeader.writeUInt32LE(16, 16); // fmt chunk size
    wavHeader.writeUInt16LE(1, 20);  // PCM format
    wavHeader.writeUInt16LE(1, 22);  // mono
    wavHeader.writeUInt32LE(Math.floor(audioBuffer.sampleRate / downsampleRate), 24); // sample rate
    wavHeader.writeUInt32LE(Math.floor(audioBuffer.sampleRate / downsampleRate) * 2, 28); // byte rate
    wavHeader.writeUInt16LE(2, 32);  // block align
    wavHeader.writeUInt16LE(16, 34); // bits per sample
    writeString(36, 'data');
    wavHeader.writeUInt32LE(audioDataBuffer.length, 40);

    // Combine header + data
    const fullWavBuffer = Buffer.concat([wavHeader, audioDataBuffer]);

    console.log(`Created WAV buffer: ${fullWavBuffer.length} bytes total`);
    return fullWavBuffer;

  } catch (error) {
    console.error(`Error in audio decode:`, error);
    throw error;
  }
}

// Convert video to audio using FFmpeg (same as transcribe API)
async function convertVideoToAudioDirect(videoBuffer: Buffer, recordingId: string): Promise<Buffer> {
  const tempVideoPath = join(tmpdir(), `video-${recordingId}-${Date.now()}.mp4`)
  const tempAudioPath = join(tmpdir(), `audio-${recordingId}-${Date.now()}.mp3`)

  try {
    // Write video buffer to temp file
    writeFileSync(tempVideoPath, videoBuffer)

    console.log(`Converting video to audio: ${tempVideoPath} -> ${tempAudioPath}`)

    // Convert video to audio using ffmpeg
    await new Promise<void>((resolve, reject) => {
      const command = ffmpeg(tempVideoPath)

      // Make sure the path is set for this command
      if (ffmpegPath) {
        command.setFfmpegPath(ffmpegPath)
      }

      command
        .audioCodec('mp3')
        .audioFrequency(16000) // 16kHz is optimal for speech recognition
        .audioChannels(1) // Mono for speech
        .audioBitrate('64k') // Lower bitrate for smaller files
        .format('mp3')
        .on('end', () => {
          console.log('Audio conversion completed for waveform')
          resolve()
        })
        .on('error', (err) => {
          console.error('FFmpeg conversion error:', err)
          reject(new Error(`Audio conversion failed: ${err.message}`))
        })
        .save(tempAudioPath)
    })

    // Read the converted audio file
    const audioBuffer = readFileSync(tempAudioPath)

    console.log(`Audio conversion successful. Original: ${videoBuffer.length} bytes, Audio: ${audioBuffer.length} bytes`)
    return audioBuffer

  } finally {
    // Clean up temp files
    try {
      unlinkSync(tempVideoPath)
      unlinkSync(tempAudioPath)
    } catch (cleanupError) {
      console.warn('Error cleaning up temp files:', cleanupError)
    }
  }
}

async function analyzeAudioBuffer(audioBuffer: Buffer, recording: Recording): Promise<WaveformData> {
  const duration = recording.duration;
  const pointsPerSecond = 5; // 5 points per second (every 0.2s) for good balance of performance and detail
  const totalPoints = Math.floor(duration * pointsPerSecond);
  const points: WaveformPoint[] = [];

  const audioData = new Uint8Array(audioBuffer);

  // Check if this is a WAV file (real audio PCM data)
  const isWAV = audioData[0] === 0x52 && audioData[1] === 0x49 && audioData[2] === 0x46 && audioData[3] === 0x46; // "RIFF"

  if (isWAV) {
    console.log(`‚úÖ REAL AUDIO: Analyzing WAV PCM data: ${audioData.length} bytes`);
    return analyzeRealWAVData(audioBuffer, recording, totalPoints, pointsPerSecond);
  } else {
    console.log(`‚ö†Ô∏è FALLBACK: Using statistical analysis on compressed data: ${audioData.length} bytes`);
    return analyzePreConvertedAudio(audioBuffer, recording, totalPoints, pointsPerSecond);
  }
}

async function analyzeRealWAVData(wavBuffer: Buffer, recording: Recording, totalPoints: number, pointsPerSecond: number): Promise<WaveformData> {
  const points: WaveformPoint[] = [];
  const wavData = new Uint8Array(wavBuffer);

  // Skip WAV header (44 bytes) to get to PCM data
  const headerSize = 44;
  const pcmData = wavData.slice(headerSize);

  console.log(`PCM data size: ${pcmData.length} bytes`);

  // WAV is 16-bit PCM at 4kHz mono, so each sample is 2 bytes
  const samplesPerPoint = Math.floor(pcmData.length / 2 / totalPoints);

  for (let i = 0; i < totalPoints; i++) {
    const time = (i / pointsPerSecond);
    const startSample = i * samplesPerPoint * 2; // * 2 because 16-bit = 2 bytes per sample
    const endSample = Math.min(startSample + (samplesPerPoint * 2), pcmData.length);

    let maxAmplitude = 0;
    let rmsSum = 0;
    let sampleCount = 0;

    // Analyze samples in this time segment
    for (let j = startSample; j < endSample; j += 2) {
      if (j + 1 < pcmData.length) {
        // Convert 16-bit PCM to amplitude (-32768 to 32767)
        const sample = (pcmData[j + 1] << 8) | pcmData[j]; // Little-endian
        const signedSample = sample > 32767 ? sample - 65536 : sample;

        // Normalize to 0-1 range
        const normalizedSample = Math.abs(signedSample) / 32768;

        maxAmplitude = Math.max(maxAmplitude, normalizedSample);
        rmsSum += normalizedSample * normalizedSample;
        sampleCount++;
      }
    }

    // Use RMS for more realistic amplitude representation
    const rms = sampleCount > 0 ? Math.sqrt(rmsSum / sampleCount) : 0;

    let amplitude = rms;

    // Smart amplitude mapping for better audio visualization
    if (amplitude < 0.005) {
      // True silence - very low amplitude
      amplitude = amplitude * 2; // Keep very quiet
    } else if (amplitude < 0.02) {
      // Background noise/very quiet speech
      amplitude = 0.01 + (amplitude - 0.005) * 3; // Minimal visibility
    } else if (amplitude < 0.1) {
      // Normal speech range
      amplitude = 0.1 + (amplitude - 0.02) * 6; // Good visibility but not maxed
    } else {
      // Loud speech/shouting - use logarithmic scaling to prevent saturation
      amplitude = 0.6 + Math.log10(amplitude * 10) * 0.2; // Cap at ~0.8-0.9 for normal loud speech
    }

    // Final clamp - reserve 1.0 for true shouting/peak audio
    amplitude = Math.max(0.001, Math.min(amplitude, 0.85));

    points.push({
      time,
      amplitude
    });
  }

  return {
    recordingId: recording.id,
    points,
    sampleRate: 4000, // We converted to 4kHz
    duration: recording.duration
  };
}

async function analyzePreConvertedAudio(audioBuffer: Buffer, recording: Recording, totalPoints: number, pointsPerSecond: number): Promise<WaveformData> {
  // Fallback for MP3/other pre-converted audio - use improved statistical analysis
  const points: WaveformPoint[] = [];
  const audioData = new Uint8Array(audioBuffer);

  const chunkSize = Math.floor(audioData.length / totalPoints);
  const headerSize = Math.min(4096, audioData.length * 0.05);
  const analysisStart = headerSize;

  for (let i = 0; i < totalPoints; i++) {
    const time = (i / pointsPerSecond);
    const startIndex = analysisStart + (i * chunkSize);
    const endIndex = Math.min(startIndex + chunkSize, audioData.length);

    if (startIndex >= audioData.length || chunkSize <= 0) {
      points.push({ time, amplitude: 0.01 });
      continue;
    }

    // Simplified analysis for MP3 data
    let variation = 0;
    let dataActivity = 0;

    for (let j = startIndex; j < endIndex; j += 10) {
      const current = audioData[j];
      const prev = audioData[j - 10] || 0;
      variation += Math.abs(current - prev);

      if (current > 50 && current < 200) {
        dataActivity += 0.1;
      }
    }

    const normalizedVariation = Math.min(variation / (endIndex - startIndex) / 50, 1.0);
    const normalizedActivity = Math.min(dataActivity, 1.0);

    let amplitude = (normalizedVariation * 0.7 + normalizedActivity * 0.3) * 0.3;
    amplitude *= (0.8 + Math.random() * 0.4);
    amplitude = Math.max(0.01, Math.min(amplitude, 0.6));

    points.push({
      time,
      amplitude
    });
  }

  return {
    recordingId: recording.id,
    points,
    sampleRate: 16000,
    duration: recording.duration
  };
}

function generateSimulatedWaveform(recording: Recording): WaveformData {
  const duration = recording.duration;
  const pointsPerSecond = 5; // 5 points per second (every 0.2s) for good balance of performance and detail
  const totalPoints = Math.floor(duration * pointsPerSecond);
  const points: WaveformPoint[] = [];

  for (let i = 0; i < totalPoints; i++) {
    const time = (i / pointsPerSecond);
    const normalizedTime = time / duration;

    // Create speech-like patterns with natural variations
    const speechCycle = Math.sin(normalizedTime * Math.PI * 8) * 0.5 + 0.5;
    const isSpeaking = speechCycle > 0.3;

    let amplitude = 0;

    if (isSpeaking) {
      const baseIntensity = 0.3 + Math.sin(normalizedTime * Math.PI * 20) * 0.2;
      const microVariations = Math.sin(normalizedTime * Math.PI * 100) * 0.1;
      const randomNoise = (Math.random() - 0.5) * 0.1;
      amplitude = Math.max(0, Math.min(1, baseIntensity + microVariations + randomNoise));
    } else {
      amplitude = Math.random() * 0.05;
    }

    points.push({
      time,
      amplitude
    });
  }

  return {
    recordingId: recording.id,
    points,
    sampleRate: 44100,
    duration
  };
}